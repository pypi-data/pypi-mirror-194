import random
from typing import Tuple, Literal, List, Dict, Callable, Union

import numpy as np
import pandas as pd
import torch
from pytorch_forecasting import metrics
from torch import nn

from . import base, datasets
from .. import preprocessing
from ..utils import rnn


class Seq2Seq(base.TimeseriesNeuralNet):
    """SeqToSeq architecture with additional Embedding layer for time
    independent features.

    This model applies a sequence to sequence learning architecture to solve
    the multivariate multistep time series forecasting problem. The additional
    embedding layer allows to condition the encoder module on
    time independent/static categorical data, making it possible to learn and
    predict multiple time series using a single model (i.e. a single non linear
    mapping).

    Parameters
    ----------
    group_ids : list of str
        List of column names identifying a time series. This means that the
        ``group_ids`` identify a sample together with the ``date``. If you
        have only one times series, set this to the name of column that is
        constant.

    time_idx : str
        Time index column. This column is used to determine the sequence of
        samples.

    target : str
        Target column. Column containing the values to be predicted.

    max_prediction_length : int
        Maximum prediction/decoder length. Usually this this is defined by
        the difference between forecasting dates.

    max_encoder_length : int
        Maximum length to encode. This is the maximum history length used by
        the time series dataset.

    time_varying_known_reals : list of str
        List of continuous variables that change over time and are known in the
        future (e.g. price of a product, but not demand of a product). If None,
        every numeric column excluding ``target`` is used.

    time_varying_unknown_reals : list of str
        List of continuous variables that change over time and are not known in
        the future. You might want to include your ``target`` here. If None,
        only ``target`` is used.

    static_categoricals : list of str
        List of categorical variables that do not change over time (also known
        as `time independent variables`). You might want to include your
        ``group_ids`` here for the learning algorithm to distinguish between
        different time series. If None, only ``group_ids`` is used.

    min_encoder_length : int, default=None
        Minimum allowed length to encode. If None, defaults to
        ``max_encoder_length``.

    criterion : class, default=None
        The uninitialized criterion (loss) used to optimize the module. If
        None, the :class:`.RMSE` (root mean squared error) is used.

    optimizer : class, default=None
        The uninitialized optimizer (update rule) used to optimize the
        module. if None, :class:`.Adam` optimizer is used.

    lr : float, default=1e-5
        Learning rate passed to the optimizer.

    max_epochs : int, default=10
        The number of epochs to train for each :meth:`fit` call. Note that you
        may keyboard-interrupt training at any time.

    batch_size : int, default=64
        Mini-batch size. If ``batch_size`` is -1, a single batch with all the
        data will be used during training and validation.

    callbacks: None, “disable”, or list of Callback instances, default=None
        Which callbacks to enable.

        - If callbacks=None, only use default callbacks which include:
            - `epoch_timer`: measures the duration of each epoch
            - `train_loss`: computes average of train batch losses
            - `valid_loss`: computes average of valid batch losses
            - `print_log`:  prints all of the above in nice format

        - If callbacks="disable":
            disable all callbacks, i.e. do not run any of the callbacks.

        - If callbacks is a list of callbacks:
            use those callbacks in addition to the default callbacks. Each
            callback should be an instance of skorch :class:`.Callback`.

    warm_start: bool, default=False
        Whether each fit call should lead to a re-initialization of the module
        (cold start) or whether the module should be trained further
        (warm start).

    emb_dim : int, default=10
        Dimension of every embedding table

    hidden_size : int, default=16
        Size of the context vector

    tf_ratio : float, default=0.2
        For every forward pass, if the sampling from a standard uniform
        distribution is less than ``tf_ratio``, teacher forcing is used.

    cell_type : str, {'lstm', 'gru}, default='lstm'
        Recurrent unit to be used for both encoder and decoder

    verbose : int, default=1
        This parameter controls how much print output is generated by
        the net and its callbacks. By setting this value to 0, e.g. the
        summary scores at the end of each epoch are no longer printed.
        This can be useful when running a hyperparameter search. The
        summary scores are always logged in the history attribute,
        regardless of the verbose setting.

    device : str, torch.device, default="cpu"
        The compute device to be used. If set to "cuda", data in torch
        tensors will be pushed to cuda tensors before being sent to the
        module. If set to None, then all compute devices will be left
        unmodified.

    kwargs : dict
       Extra prefixed parameters (see list of supported prefixes with
       self.prefixes).
    """

    def __init__(
            self, group_ids: List[str], time_idx: str, target: str,
            max_prediction_length: int, max_encoder_length: int,
            time_varying_known_reals: List[str],
            time_varying_unknown_reals: List[str],
            static_categoricals: List[str], min_encoder_length: int = None,
            criterion: metrics.MultiHorizonMetric = metrics.RMSE,
            optimizer: torch.optim.Optimizer = torch.optim.Adam,
            train_split: Callable = None, lr: float = 1e-5,
            max_epochs: int = 10, batch_size: int = 64,
            callbacks: List = None, warm_start: bool = False, emb_dim: int = 10,
            hidden_size: int = 16, tf_ratio: float = 0.2,
            cell: Literal["lstm", "gru"] = "lstm", num_layers: int = 1,
            verbose: int = 1, device: Literal["cpu", "cuda"] = "cpu",
            **kwargs
    ):
        super().__init__(
            module=Seq2SeqModule, dataset=datasets.TimeseriesDataset,
            group_ids=group_ids, time_idx=time_idx, target=target,
            max_prediction_length=max_prediction_length,
            time_varying_known_reals=time_varying_known_reals,
            time_varying_unknown_reals=time_varying_unknown_reals,
            static_categoricals=static_categoricals,
            max_encoder_length=max_encoder_length,
            min_encoder_length=min_encoder_length,
            max_epochs=max_epochs, lr=lr, batch_size=batch_size,
            optimizer=optimizer, train_split=train_split, criterion=criterion,
            callbacks=callbacks, collate_fn=Seq2SeqCollateFn(),
            output_decoder=Seq2SeqOutputDecoder, warm_start=warm_start,
            verbose=verbose, device=device, **kwargs
        )
        self.emb_dim = emb_dim
        self.hidden_size = hidden_size
        self.tf_ratio = tf_ratio
        self.cell = cell
        self.num_layers = num_layers


class Seq2SeqOutputDecoder(base.BaseOutputDecoder):
    """Decodes Seq2Seq output.

    Inverse transforms output sliding sequences and places in a
    pandas DataFrame (if given)

    Parameters
    ----------
    dataset : datasets.TimeseriesDataset
        Predicted dataset.
    """

    def __init__(self, dataset: datasets.TimeseriesDataset):
        super().__init__(dataset)

    def decode(
            self,
            output: np.ndarray,
            out: pd.DataFrame = None
    ) -> Union[np.ndarray, pd.DataFrame]:
        """Decodes outputs.

        Parameters
        ----------
        output : np.array
            Output to decode.

        out : pd.DataFrame, default=None
            Destination to place the result.

        Returns
        -------
        decoded_output : np.array or pd.DataFrame
            If given, results will be placed in ``out`` under a new column
            called "output".
        """
        sequences = {}
        groupby = self.dataset.decoded_index.groupby(self.dataset.group_ids)

        for i, group_id in enumerate(groupby.groups):
            index = groupby.get_group(group_id).index
            group_sliding_sequences = output[index]
            group_sequence = self._inverse_transform_sliding_sequences(
                group_sliding_sequences)
            sequences[group_id] = group_sequence

        if out is not None:
            return self._place_sequences(out, self.dataset.target, sequences)
        return sequences

    def _place_sequences(
            self,
            X: pd.DataFrame,
            column: str,
            sequences: Dict[str, np.array]
    ):
        """Places output sequences in X.

        Parameters
        ----------
        X : pd.DataFrame
            DataFrame to place the sequences.

        column : str
            Column in ``X`` to place the sequences.

        sequences : dict, str -> np.array
            Mapping from group_id -> sequence.

        """

        def apply_func(group):
            """Creates a new column containing the output sequence.

            Parameters
            ----------
            group : pd.DataFrame


            Returns
            -------
            group : pd.DataFrame
            """
            if group.name not in sequences:
                return group
            group_sequence = sequences[group.name].flatten()
            sequence_length = len(group_sequence)
            group = group.iloc[-sequence_length:]
            group[column] = group_sequence
            return group

        groupby = X.groupby(self.dataset.group_ids, group_keys=False)
        return groupby.apply(apply_func)

    def _inverse_transform_sliding_sequences(self, sliding_sequences):
        return preprocessing.inverse_transform_sliding_sequences(
            sliding_sequences=sliding_sequences,
            sequence_length=self.dataset.max_prediction_length)


class Seq2SeqModule(base.BaseModule):
    """Encoder-decoder architecture applied to timeseries.

    An encoder network condenses an input sequence into a vector,
    and a decoder network unfolds that vector into a new sequence.

    Additionally, this model is equipped with an additional embedding layer for
    conditioning the Encoder unit with time independent features.

    Parameters
    ----------
    emb_dim : int
        Dimension for every embedding table.

    emb_sizes : tuple
        Size of each embedding table. For example, if two static categorical
        features are to be included to the model and they have `n` and `m`
        unique elements, respectively, then ``emb_sizes`` must equal `(n, m)`.
        The dimension of each embedding table is given by the ``emb_dim``
        param.

    enc_size : int
        Encoder size. Number of features to be included in the encoder module

    dec_size : int
        Decoder size. Number of feature to be included in the decoder module

    hidden_size : int
        Size of the context vector

    tf_ratio : float, default=0.2
        For every forward pass, if the sampling from a standard uniform
        distribution is less than ``tf_ratio``, teacher forcing ratio will be
        used.

    cell : str, {'lstm', 'gru}, default='lstm'
    """

    def __init__(
            self, emb_dim: int, emb_sizes: Tuple[int], enc_size: int,
            dec_size: int, hidden_size: int, tf_ratio: float = 0.2,
            cell: Literal["lstm", "gru"] = "lstm", num_layers: int = 1
    ):
        super().__init__()
        self.embedder = Embedder(emb_sizes, emb_dim, hidden_size, num_layers)
        self.encoder = Encoder(cell, enc_size, hidden_size, num_layers)
        self.decoder = Decoder(cell, dec_size, hidden_size, num_layers,
                               tf_ratio)

    def forward(self, emb_x, enc_x, dec_x, dec_y, enc_lens):
        """Forwards pass.

        Notice encoder's final state corresponds to decoder's initial state.
        """
        enc_state = self.embedder(emb_x)
        enc_out, dec_state = self.encoder(enc_x, enc_lens, state=enc_state)
        return self.decoder(dec_x, dec_state, enc_out, dec_y)

    @classmethod
    def from_dataset(cls, dataset: datasets.TimeseriesDataset, **kwargs):
        """Creates model from dataset.

        Parameters
        ----------
        dataset : TimeseriesDataset
            Dataset object from which init parameters will be obtained.

        kwargs : keyword arguments
            Additional arguments such as hyperparameters for model
            (see ``__init__()``).

        Returns
        -------
        SeqToSeqModule object
        """
        # Get sizes from dataset.
        # The +1 is for unseen categorical data.
        emb_sizes = torch.max(dataset.data['categoricals'], dim=0)
        emb_sizes = tuple(emb_sizes.values.numpy() + 1)
        enc_size = len(
            dataset.time_varying_known_reals +
            dataset.time_varying_unknown_reals
        )
        dec_size = len(
            dataset.time_varying_known_reals +
            [dataset.target]
        )
        return cls(emb_sizes=emb_sizes, enc_size=enc_size, dec_size=dec_size,
                   **kwargs)


class Embedder(nn.Module):
    """Embedding layer.

    Conditions the Encoder with time independents/static categorical variables.
    """

    def __init__(self, emb_sizes, emb_dim, hidden_size, num_layers):
        super().__init__()
        self.emb_sizes = emb_sizes
        self.emb_dim = emb_dim
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        emb_trans = {}
        linear_trans = {}
        self.linear = nn.Linear(hidden_size, len(emb_sizes))
        for i, size in enumerate(emb_sizes):
            # Each column will have its own embedding and linear
            # transformation. The order will be kept through dictionaries.
            emb_trans[str(i)] = nn.Embedding(size, emb_dim)
            linear_trans[str(i)] = nn.Linear(emb_dim, hidden_size * num_layers)
        self.emb_trans = nn.ModuleDict(emb_trans)
        self.linear_trans = nn.ModuleDict(linear_trans)

    def forward(self, tokens):
        batch_size = tokens.shape[0]

        # Pre-allocate memory for output.
        n_emb = len(self.emb_sizes)
        output_shape = (batch_size, self.hidden_size, self.num_layers, n_emb)
        dense_output = torch.zeros(output_shape, device=tokens.device)

        for i, col in enumerate(self.emb_trans):
            column_tokens = tokens[:, [i]]
            emb_column = self.emb_trans[col](column_tokens)
            dense_column = self.linear_trans[col](emb_column)
            dense_output[:, :, :, i] = torch.reshape(
                dense_column.squeeze(dim=1),
                (batch_size, self.hidden_size, self.num_layers)
            )

        if len(self.emb_sizes) > 1:
            output = self.linear(dense_output)
        else:
            output = dense_output

        output = output.squeeze(dim=-1).permute(2, 0, 1)
        return output


class Encoder(nn.Module):
    """Encoder module.

    Condenses an input sequence into a vector of fixed length
    (the context vector).

    Parameters
    ----------
    cell : str, {'lstm', 'gru'}
        Rnn cell for encoder and decoder

    input_size : int
        Decoder input size (number of features).

    hidden_size : int
        Size of context vector.

    num_layers : int
        Number of hidden layers.
    """

    def __init__(self, cell, input_size, hidden_size, num_layers):
        super().__init__()
        self.rnn = rnn.make_rnn(cell, input_size, hidden_size, num_layers)

    def forward(self, X, enc_lens, state):
        """Encoder forward pass.
        """
        if isinstance(self.rnn, nn.LSTM):
            c0 = torch.zeros_like(state, device=state.device)
            state = state, c0

        X = self._pack_padded(X, enc_lens)
        output, state = self.rnn(X, state)
        output, _ = self._pad_packed(output)
        return output, state

    def _pack_padded(self, X, lenghts):
        """
        """
        return rnn.pack_padded_sequence(
            input=X, lengths=lenghts, batch_first=True, enforce_sorted=False)

    def _pad_packed(self, sequence):
        """
        """
        return rnn.pad_packed_sequence(sequence=sequence, batch_first=True)


class Decoder(nn.Module):
    """Decoder module.

    Parameters
    ----------
    cell : str, {'lstm', 'gru'}
        Rnn cell for encoder and decoder

    input_size : int
        Decoder input size (number of features).

    hidden_size : int
        Size of context vector.

    num_layers : int
        Number of hidden layers.

    tf_ratio : float
        For each forward pass, if a random sample from a standard uniform
        distribution is less than `tf_ratio`, teacher forcing is used. Use 0
        if teacher forcing is not desired at all.
    """

    def __init__(self, cell, input_size, hidden_size, num_layers, tf_ratio):
        super().__init__()
        self.rnn = rnn.make_rnn(cell, input_size, hidden_size, num_layers)
        self.tf_ratio = tf_ratio
        self.linear = nn.Linear(in_features=hidden_size, out_features=1)

    def forward(self, X, state, enc_out, y):
        """Decoder forward pass.
        """
        X = self._select_features(X)
        use_teacher = self._use_teacher_forcing()
        output = self._forward(use_teacher, X, state, enc_out, y)
        return output.squeeze(-1)  # Squeeze last dim from linear output.

    def _select_features(self, X):
        """Only the first ``input_size - 1`` features are selected.
        """
        target_size = 1
        features_size = self.rnn.input_size - target_size
        return X[:, :, :features_size]

    def _use_teacher_forcing(self):
        """Decides if teacher forcing is used.

        Two conditions must be satisfied:
        1. Sampling from the standard uniform random must be less than tf_ratio.
        2. model.train() is active (This prevents its usage when evaluating
        the model).

        Returns
        -------
        bool
        """
        cond1 = True if random.random() < self.tf_ratio else False
        cond2 = self.training
        return cond1 and cond2

    def _forward(self, use_teacher, dec_x, state, enc_out, y):
        if use_teacher:
            return self._with_teacher(y, dec_x, state)
        return self._without_teacher(enc_out, dec_x, state)

    def _with_teacher(self, y, dec_x, state):
        """Teacher forcing forward pass.

        Target values are used as input at every step.
        This is achieved by concatenating both the target values `y` and the
        features values `dec_x` into a single input tensor.
        """
        # ``y`` is a tensor of size (batch_size, T) where T is
        # the prediction length and ``dec_x`` is a tensor of size
        # (batch_size, T, F) where F is the number of features. Therefore,
        # an extra dimension is added to ``y`` to perform torch.cat and produce
        # the cell input.
        dec_input = torch.cat((y.unsqueeze(dim=2), dec_x), dim=2)
        dec_output, state = self.rnn(dec_input, state)
        return self.linear(dec_output)

    def _without_teacher(self, enc_out, dec_x, state):
        """Without teacher forcing forward pass.

        Output values are used as next input at every step with the first
        input being the last encoder output.
        """
        batch_size = enc_out.shape[0]
        output_length = dec_x.shape[1]

        # First decoder input is the last encoder output
        dec_input_t = enc_out[:, -1:, -1:]

        # Pre-allocate memory for output.
        dec_output = torch.zeros(
            (batch_size, output_length, 1), device=dec_x.device)

        for t in range(output_length):
            future_seqs_t = dec_x[:, [t], :]
            dec_input_t = torch.cat((dec_input_t, future_seqs_t), dim=2)
            dec_output_t, state = self.rnn(dec_input_t, state)
            dense_dec_output_t = self.linear(dec_output_t)
            dec_output[:, t, :] = dense_dec_output_t.squeeze(1)
            dec_input_t = dense_dec_output_t.detach()

        return dec_output


class Seq2SeqCollateFn:

    def __call__(
            self,
            batch: List[Tuple[Dict[str, torch.Tensor], torch.Tensor]]
    ) -> Tuple[Dict[str, torch.Tensor], torch.Tensor]:
        """Collate function to combine items into mini-batch for dataloader.

        Parameters
        ----------
        batch : List[Tuple[Dict[str, torch.Tensor], torch.Tensor]]:
            List of samples.


        Returns
        -------
        minibatch : Tuple[Dict[str, torch.Tensor], torch.Tensor]
        """
        batch_size = len(batch)

        # Init arrays.
        # Pre-allocate memory whenever possible.
        emb_x = torch.zeros(batch_size, 1, dtype=int)
        encoder_lengths = []
        enc_x = []
        dec_x = []
        Y = []

        for i, (X, y) in enumerate(batch):
            length = X['encoder_length']
            encoder_lengths.append(length)

            emb_x[i] = X['x_cat'][0]
            enc_x.append(X['x_cont'][:length])
            dec_x.append(X['x_cont'][length:])
            Y.append(y[0])

        enc_x = rnn.pad_sequence(enc_x, batch_first=True)
        dec_x = torch.stack(dec_x)
        Y = torch.stack(Y)

        X = {
            'emb_x': emb_x,
            'enc_x': enc_x,
            'dec_x': dec_x,
            'dec_y': Y,
            'enc_lens': encoder_lengths
        }

        return X, Y
