

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>User Guide &mdash; Responsible AI Tracker  documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="_static/sg_gallery.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Responsible AI Tracker
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="install_guide.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="basics_disaggregated.html">Basics of Disaggregated Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="how.html">How This Extension Works With Responsible AI Toolbox</a></li>
<li class="toctree-l1"><a class="reference internal" href="tour.html">Tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="frequently_asked.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="known_issues.html">Known Issues</a></li>
<li class="toctree-l1"><a class="reference internal" href="get_involved.html">Get Involved</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="project_management.html">Project Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebook_model_management.html">Notebook and Model Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="cohort_management.html">Cohort Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="creating_mitigations.html">Creating Mitigations</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_comparison.html">Model Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="integration_mlflow.html">Integration with MLFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="metric_support.html">Metric Support</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Responsible AI Tracker</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>User Guide</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            
              <a href="https://github.com//microsoft/responsible-ai-toolbox-tracker/blob/main/docs/user_guide.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="user-guide">
<span id="id1"></span><h1>User Guide<a class="headerlink" href="#user-guide" title="Permalink to this heading">¶</a></h1>
<section id="project-management">
<h2>Project Management<a class="headerlink" href="#project-management" title="Permalink to this heading">¶</a></h2>
<p>Projects can be created through the main menu of the extension. Creating a new project will result in creating a dedicated project folder in the Responsible AI Tracker workspace. Currently, the extension supports either classification or regression projects. Depending on the type of project selected during project creation, the metrics shown and tracked in the model comparison table will change to either classification or regression metrics. It is also possible to optionally upload an initial notebook to the project.</p>
<p>Project deletion will lead to deleting all related artifacts and the project folder from the workspace.</p>
<p>TODO gif with project creation</p>
</section>
<section id="notebook-and-model-management">
<h2>Notebook and Model management<a class="headerlink" href="#notebook-and-model-management" title="Permalink to this heading">¶</a></h2>
<p>The current tracking model of the extension enables mapping models to notebooks. This happens through the process of model registration. Exactly one model can be registered to one notebook. This is to encourage clean code organization and model management practices so that if it becomes necessary to go back and understand the code that created a given model, practitioners can directly access the corresponding notebook. If there is a need to create code that is reused across different models and notebooks, we encourage practitioners to save these in separate python files and keep in the notebooks only code that is specific to the registered model.</p>
<p>Notebooks can be either directly created or imported from notebooks outside of the project workspace. It is also possible to duplicate a notebook from an existing one. During model registration, the registration process will also ask for the corresponding test dataset, which will then be used to score the model. All test datasets that have been used in at least one registered model can also be used to create cohorts later on.</p>
<p>TODO gif with creating a notebook and registering a model on that notebook.</p>
</section>
<section id="cohort-management">
<h2>Cohort Management<a class="headerlink" href="#cohort-management" title="Permalink to this heading">¶</a></h2>
<p>Cohorts can be directly created, managed, or deleted through the cohort management interface. A cohort can be created from any overall test dataset. Creating a cohort from a test dataset entails adding filters to the test dataset. All added filters are then combined together through conjunctions (AND operators). The interface does not currently support disjunctions of filters (OR operators).</p>
<p>Upon cohort creation, all models that have been registered using the given test dataset will then also be scored on the newly created cohort. This will result in creating a new row in the model comparison table for each affected model, displaying performance metrics of the model on the new cohort.</p>
<p>TODO gif with creating a new cohort and showing how that changes the model comparison table.</p>
</section>
<section id="creating-mitigations">
<h2>Creating Mitigations<a class="headerlink" href="#creating-mitigations" title="Permalink to this heading">¶</a></h2>
<p>Each new notebook created in the project should ideally correspond to a mitigation attempt or experiment that is different from previous experiments. Mitigation experiments may involve data improvements, changes in the model class or architecture, changes in the loss function, hyperparameter selection, or changes in the optimization process (e.g., regularization, choice of optimization algorithms and methods).</p>
<p>Tracking notebooks and registering models to notebooks accordingly enables tracking (in code) which types of mitigations have been applied in different mitigation experiments.</p>
<p>The lightbulb button in the notebook menu will give you some initial ideas to explore on mitigation techniques from the Responsible AI Mitigations library. Some notable mitigation examples from raimitigation include data balancing and synthesis, feature engineering, missing value imputation. Most importantly, the library also simplifies the process of programmatically applying different data mitigation steps to different cohorts, in cases when the underlying issues for model errors are specific to those cohorts.</p>
<p>TODO gif with creating a new notebook and using the lightbulb button and explore the different mitigations.</p>
</section>
<section id="model-comparison">
<h2>Model Comparison<a class="headerlink" href="#model-comparison" title="Permalink to this heading">¶</a></h2>
<p>The model comparison table can be shown by clicking the “Compare Models” button. The notebook and model created first will serve as a baseline to the comparison. The table will compare models through different metrics and across created cohorts. It is also possible to restrict the table to only a set of selected notebooks, metrics or cohorts of choice. The list of notebooks, metrics, and cohorts can be customized in their respective dropdowns on top of the table. Only notebooks that have a registered model will be shown.</p>
<p>The table has two views: absolute and comparative. The absolute view will show raw absolute score metrics and will be shaded using one single color. Generally, it is recommended to use a stronger shade for desirable performance. The comparative view will also show the corresponding differences between model performance and the baseline performance either for the overall dataset or for the given cohort. For example, if the accuracy of the baseline is 0.8 in the overall dataset and the accuracy of a mitigated model is 0.85 for the overall dataset, the respective cell in the table will show 0.85 (0.05 ↑), indicating that there is a 0.05 improvement for the overall dataset. Similarly, if the accuracy of the baseline for the same baseline is instead 0.9 for cohort A, but it is 0.87 for the newly mitigated model, the respective cell for the model and cohort A will show 0.87 (0.03 ↓) indicating a 0.03 decline in accuracy for cohort A. This enables a 1:1 comparison across cohorts over several models. The shading in the comparative view is based on two colors: one for performance improvement and one for performance decline.</p>
<p>TODO gif with showing the model comparison table in both modes: absolute and comparative.</p>
<p>Some mitigation steps might make test datasets incompatible with mitigated models. For example, mitigation steps such as feature encoding or feature selection may change the number of features in the test dataset, while the number of records (rows) remains the same. While Responsible AI Tracker is not able to track these changes automatically, it is still possible to register a model with an adjusted test dataset. Note that the cohort definitions created from one test dataset will not transfer to another one, even if the number of records remains the same. In this case, we recommend that corresponding cohorts are created again through the cohort management interface using now the adjusted test dataset. Afterwards, it is possible to enable a 1:1 comparison of the adjusted cohort with the original one using the overflow menu (…) for that cohort in the model comparison table.</p>
<p>TODO gif showing the overflow menu where we directly choose to compare a cohort with another one from the baseline.</p>
</section>
<section id="integration-with-mlflow">
<h2>Integration with MLFlow<a class="headerlink" href="#integration-with-mlflow" title="Permalink to this heading">¶</a></h2>
<p>All models registered in the extension are also registered in a local deployment of mlflow.</p>
<p>TODO: Besa to work with Dany in writing down this text. Besa and Dany</p>
</section>
<section id="metrics-supported">
<h2>Metrics Supported<a class="headerlink" href="#metrics-supported" title="Permalink to this heading">¶</a></h2>
<p>Responsible AI Tracker reports the following metrics depending on the prediction problem:</p>
<p>Classification: accuracy, precision, recall, F1, logloss</p>
<p>Regression: mean-squared error (mse), root mean-squared error (rmse), mean absolute error (mae), r-squared (r2)</p>
<p>TODO: add link above to the library we are using to compute these metrics specifically. In this case sklearn.</p>
<p>TODO: add note here to describe whether we are taking a macro vs micro approach on computing metrics.</p>
</section>
</section>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2023, Microsoft.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>